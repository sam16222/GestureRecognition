{
  "name": "GestureAR",
  "tagline": "Robust hand gestural interaction for smartphone based applications",
  "body": "### Welcome to GestAR\r\nThe future of UI in AR and VR applications will be dominated by hand gestures. In this paper, we are exploring an intuitive hand-gesture based interaction for smartphones having a limited computational capability. To this end, we present an efficient algorithm for gesture recognition with First Person View (FPV) that focuses on recognizing a four swipe model (Left, Right, Up and Down) for smartphones through single monocular camera vision. This can be used with frugal AR/VR devices such as Google Cardboard(https://vr.google.com/cardboard/) and Wearality(http://www.wearality.com/) in building AR/VR based automation systems, for large scale deployments by providing a touch-less interface with real-time performance. We take into account multiple cues, including palm color, hand contour segmentation, and motion tracking which effectively deals with FPV constraints caused due to a wearable device. \r\n\r\n### The Idea\r\n\r\n![Algorithmic Flow Diagram ](https://github.com/sam16222/GestureRecognition/blob/master/img/Image-Flow-Diagram.png?raw=true?)\r\n\r\nWe propose a technique for hand gesture classification by categorising the hand motion in four type of swipes such as Up, Down, Left and Right which can be used for triggering different types of events which are intuitive for wearable AR/VR gadgets. We circumvent the shortcomings of the probabilistic models for hand detection by using a simpler palm detection model that works purely on **Cb** and **Cr** values using a statistical model. The step generates multiple blobs on the palm segment. Contour detection filters the blobs that are small and hence would not correspond to palm. A Shi-Tomasi detector based simple approach is applied for feature point detection on the contour boundary. The motion of these feature points is analysed by computing the optical flow based displacement. The aggregate measure of the displacement is used for labelling the swipe under major groups. The approach is computationally simple and can be run on an ordinary smart-phone. We demonstrate the performance of this approach on more than hundred of sample video segments with over _95%_ classification accuracy. This technique can be easily integrated with many applications for the overlaid information traversal spanning multiple pages. \r\n\r\n### Authors and Contributors\r\nShreyash Mohatta (@sam16222), Ramakrishna Perla, Gaurav Gupta, Ehtesham Hassan, Ramya Hebbalaguppe.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and weâ€™ll help you sort it out.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}